# Speech-to-Text Implementation Documentation

## üéØ What I Did

### 1. Environment Setup
Created standardized environment configuration:

**Files Created:**
- `.env.copy` - Template with all required API keys and settings
- `.env` - Actual environment file (you need to add your API keys)

**Key Environment Variables:**
```bash
DEEPGRAM_API_KEY=your_deepgram_api_key_here
DEEPGRAM_MODEL=nova-2
DEEPGRAM_LANGUAGE=en-US
DEEPGRAM_SMART_FORMAT=true
```

### 2. Package Dependencies
Updated `package.json` with required dependencies:
- `@deepgram/sdk`: ^3.4.0 - Official DeepGram JavaScript SDK
- `dotenv`: ^16.4.5 - Environment variable management

### 3. Code Architecture

Created modular speech-to-text service in `src/services/speech/`:

#### File Structure:
```
src/services/speech/
‚îú‚îÄ‚îÄ speechToText.js    # Main coordinator service
‚îú‚îÄ‚îÄ deepgram.js        # DeepGram WebSocket integration
‚îú‚îÄ‚îÄ microphone.js      # Browser microphone handling
‚îî‚îÄ‚îÄ index.js          # Export barrel
```

#### Core Components:

**A. DeepgramService (`deepgram.js`)**
- Manages WebSocket connection to DeepGram API
- Handles real-time audio streaming
- Processes transcription events
- Auto-reconnection logic

**B. MicrophoneService (`microphone.js`)**
- Browser microphone access via MediaDevices API
- Real-time audio recording with MediaRecorder
- Audio chunk streaming (250ms intervals)
- Audio level visualization support

**C. SpeechToTextService (`speechToText.js`)**
- Main coordinator between microphone and DeepGram
- Event callback system
- Error handling and auto-restart
- Status management

### 4. Key Features Implemented

#### Real-time Processing:
- ‚úÖ Live audio capture from microphone
- ‚úÖ Streaming to DeepGram via WebSocket
- ‚úÖ Interim and final transcript results
- ‚úÖ Automatic silence detection

#### Error Handling:
- ‚úÖ Connection loss recovery
- ‚úÖ Auto-restart on network errors
- ‚úÖ Graceful cleanup of resources
- ‚úÖ Status tracking

#### Configuration:
- ‚úÖ Configurable audio settings
- ‚úÖ DeepGram model selection
- ‚úÖ Smart formatting options
- ‚úÖ Language selection

## üß™ How to Test the Implementation

### Step 1: Install Dependencies
```bash
npm install
```

### Step 2: Get DeepGram API Key
1. Sign up at [https://console.deepgram.com/](https://console.deepgram.com/)
2. Create a new project
3. Generate an API key
4. Add it to your `.env` file:
```bash
DEEPGRAM_API_KEY=your_actual_api_key_here
```

### Step 3: Create Test HTML File
Create `test/speech-test.html`:

```html
<!DOCTYPE html>
<html>
<head>
    <title>Speech-to-Text Test</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        .container { max-width: 600px; margin: 0 auto; }
        .transcript { 
            border: 1px solid #ccc; 
            padding: 15px; 
            height: 200px; 
            overflow-y: auto; 
            background: #f9f9f9;
        }
        .controls { margin: 20px 0; }
        button { padding: 10px 20px; margin: 5px; }
        .status { 
            padding: 10px; 
            margin: 10px 0; 
            border-radius: 4px;
        }
        .active { background: #d4edda; color: #155724; }
        .error { background: #f8d7da; color: #721c24; }
    </style>
</head>
<body>
    <div class="container">
        <h1>Speech-to-Text Test</h1>
        
        <div class="controls">
            <button id="startBtn">Start Listening</button>
            <button id="stopBtn" disabled>Stop Listening</button>
            <button id="clearBtn">Clear Transcript</button>
        </div>
        
        <div id="status" class="status">Ready to start</div>
        
        <h3>Live Transcript:</h3>
        <div id="transcript" class="transcript"></div>
        
        <h3>Final Results:</h3>
        <div id="finalResults" class="transcript"></div>
    </div>

    <script type="module">
        import { SpeechToTextService } from '../src/services/speech/index.js';
        
        // Initialize service
        const speechService = new SpeechToTextService('YOUR_DEEPGRAM_API_KEY');
        
        // DOM elements
        const startBtn = document.getElementById('startBtn');
        const stopBtn = document.getElementById('stopBtn');
        const clearBtn = document.getElementById('clearBtn');
        const status = document.getElementById('status');
        const transcript = document.getElementById('transcript');
        const finalResults = document.getElementById('finalResults');
        
        // Set up callbacks
        speechService.setCallbacks({
            onTranscript: (result) => {
                if (!result.isFinal) {
                    transcript.innerHTML = `<em>${result.text}</em>`;
                }
            },
            
            onFinalTranscript: (result) => {
                const div = document.createElement('div');
                div.innerHTML = `
                    <strong>[${result.timestamp}]</strong> ${result.text}
                    <small>(confidence: ${(result.confidence * 100).toFixed(1)}%)</small>
                `;
                finalResults.appendChild(div);
                finalResults.scrollTop = finalResults.scrollHeight;
                transcript.innerHTML = '';
            },
            
            onError: (error) => {
                status.textContent = `Error: ${error.message}`;
                status.className = 'status error';
            },
            
            onStatusChange: (statusType) => {
                status.textContent = `Status: ${statusType}`;
                status.className = statusType === 'active' ? 'status active' : 'status';
            }
        });
        
        // Event listeners
        startBtn.addEventListener('click', async () => {
            try {
                await speechService.start();
                startBtn.disabled = true;
                stopBtn.disabled = false;
            } catch (error) {
                alert(`Failed to start: ${error.message}`);
            }
        });
        
        stopBtn.addEventListener('click', () => {
            speechService.stop();
            startBtn.disabled = false;
            stopBtn.disabled = true;
        });
        
        clearBtn.addEventListener('click', () => {
            transcript.innerHTML = '';
            finalResults.innerHTML = '';
        });
        
        // Cleanup on page unload
        window.addEventListener('beforeunload', () => {
            speechService.cleanup();
        });
    </script>
</body>
</html>
```

### Step 4: Create Simple Node.js Test
Create `test/node-test.js`:

```javascript
import 'dotenv/config';
import { DeepgramService } from '../src/services/speech/index.js';

async function testDeepgramConnection() {
    const deepgram = new DeepgramService(process.env.DEEPGRAM_API_KEY);
    
    console.log('Testing DeepGram connection...');
    
    try {
        await deepgram.startListening(
            {}, 
            (transcript) => {
                console.log('Transcript:', transcript);
            },
            (error) => {
                console.error('Error:', error);
            }
        );
        
        console.log('‚úÖ DeepGram connection successful');
        
        // Simulate some audio data (you'd replace this with real audio)
        setTimeout(() => {
            deepgram.stopListening();
            console.log('Test completed');
        }, 5000);
        
    } catch (error) {
        console.error('‚ùå DeepGram connection failed:', error);
    }
}

testDeepgramConnection();
```

### Step 5: Test with Local Server
Create `test/server.js`:

```javascript
import express from 'express';
import path from 'path';

const app = express();
const PORT = 3000;

// Serve static files
app.use(express.static('.'));

// Serve test page
app.get('/test', (req, res) => {
    res.sendFile(path.resolve('test/speech-test.html'));
});

app.listen(PORT, () => {
    console.log(`Test server running at http://localhost:${PORT}/test`);
});
```

### Step 6: Run Tests

**Option A: Browser Test**
```bash
# Start test server
node test/server.js

# Open browser to: http://localhost:3000/test
# Click "Start Listening" and speak into microphone
```

**Option B: Node.js Test**
```bash
node test/node-test.js
```

## üîß Testing Checklist

### ‚úÖ Basic Functionality
- [ ] Microphone permission granted
- [ ] DeepGram connection established
- [ ] Real-time transcription working
- [ ] Final transcript accuracy
- [ ] Start/stop controls working

### ‚úÖ Error Handling
- [ ] Invalid API key handling
- [ ] Network disconnection recovery
- [ ] Microphone access denied
- [ ] Auto-restart functionality

### ‚úÖ Performance
- [ ] Low latency transcription
- [ ] Stable WebSocket connection
- [ ] Memory usage reasonable
- [ ] Audio quality good

## üêõ Troubleshooting

**Common Issues:**

1. **"Microphone access denied"**
   - Solution: Allow microphone permissions in browser
   - Check browser security settings

2. **"DeepGram connection failed"**
   - Solution: Verify API key in `.env` file
   - Check internet connection

3. **"No transcription results"**
   - Solution: Speak clearly and loudly
   - Check microphone is working
   - Verify audio format compatibility

4. **Module import errors**
   - Solution: Use ES6 modules or update to CommonJS
   - Check file paths and exports

## üéØ Next Steps

1. **Integration with AI Agents**: Connect final transcripts to your dialogue system
2. **UI Components**: Create React/Vue components for the interface
3. **Voice Activity Detection**: Add silence detection
4. **Multi-language Support**: Test with different languages
5. **Performance Optimization**: Implement audio compression

## üìÅ Files Created Summary

```
üìÅ Project Structure Added:
‚îú‚îÄ‚îÄ .env.copy                     # Environment template
‚îú‚îÄ‚îÄ .env                         # Environment variables
‚îú‚îÄ‚îÄ src/services/speech/
‚îÇ   ‚îú‚îÄ‚îÄ speechToText.js          # Main service
‚îÇ   ‚îú‚îÄ‚îÄ deepgram.js             # DeepGram integration
‚îÇ   ‚îú‚îÄ‚îÄ microphone.js           # Microphone handling
‚îÇ   ‚îî‚îÄ‚îÄ index.js                # Exports
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îî‚îÄ‚îÄ speech-to-text.md       # This documentation
‚îî‚îÄ‚îÄ package.json                # Updated dependencies
```

This implementation provides a solid foundation for the "User speaks ‚Üí DeepGram ‚Üí Text" pipeline in your AI dialogue project!
